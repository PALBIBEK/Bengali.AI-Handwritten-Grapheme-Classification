{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTU01ZcvqC179AgVObEkrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PALBIBEK/Bengali.AI-Handwritten-Grapheme-Classification/blob/main/TF_IDf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 1: Compute TF-IDF Matrix Manually\n",
        "Objective: Learn how to compute the TF-IDF matrix from scratch.\n",
        "Task:\n",
        "Create a small corpus of at least 5 documents with varying lengths.\n",
        "Calculate the term frequency (TF) for each term in each document.\n",
        "Compute the inverse document frequency (IDF) for each term.\n",
        "Calculate the TF-IDF for each term in each document using the formula:\n",
        "TF-IDF\n",
        "(\n",
        "ùë°\n",
        ",\n",
        "ùëë\n",
        ")\n",
        "=\n",
        "TF\n",
        "(\n",
        "ùë°\n",
        ",\n",
        "ùëë\n",
        ")\n",
        "√ó\n",
        "IDF\n",
        "(\n",
        "ùë°\n",
        ")\n",
        "TF-IDF(t,d)=TF(t,d)√óIDF(t)\n",
        "Display the resulting TF-IDF matrix.**"
      ],
      "metadata": {
        "id": "H_8-uu2dy_zK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bnW-FnVPJzud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzIDQvgTeq7i",
        "outputId": "c54b5b81-64b1-481e-b687-af4a86e5ad72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     doc1      doc2      doc3    doc4\n",
            "0  0.0000  0.000000  0.115500  0.0000\n",
            "1  0.0000  0.000000  0.000000  0.0000\n",
            "2  0.0576  0.000000  0.000000  0.0576\n",
            "3  0.0000  0.000000  0.000000  0.0000\n",
            "4  0.0000  0.000000  0.115500  0.0000\n",
            "5  0.0000  0.115500  0.000000  0.0000\n",
            "6 -0.0892 -0.037167 -0.037167 -0.0446\n",
            "7  0.0000  0.000000  0.115500  0.0000\n",
            "8 -0.0446 -0.037167 -0.037167 -0.0446\n"
          ]
        }
      ],
      "source": [
        "from functools import reduce\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def TF(term , document):\n",
        "  words=document.lower().split()\n",
        "  # count_of_term=len(reduce(lambda x,i: x + [words[i]] if words[i]==term else x,range(0,len(words)),[]))\n",
        "  count_of_term=len(list(filter(lambda x: x==term, words)))\n",
        "  return count_of_term/len(words)\n",
        "\n",
        "corpus = [\n",
        "    \"this the the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\",\n",
        "]\n",
        "all_tokens=set()\n",
        "for doc in corpus:\n",
        "  all_tokens.update(doc.split())\n",
        "all_tokens= sorted(all_tokens,key=lambda x: x.lower())\n",
        "TF_IDF_matrix=[ [ 0 for _ in corpus] for _ in all_tokens]\n",
        "for i, word in enumerate(all_tokens):\n",
        "  for j,doc in enumerate(corpus):\n",
        "    TF_IDF_matrix[i][j]=TF(word,doc)\n",
        "\n",
        "frequency=[0 for _ in all_tokens]\n",
        "\n",
        "for i,word in enumerate(all_tokens):\n",
        "  for doc in corpus:\n",
        "    words=doc.lower().split()\n",
        "    if word in words:\n",
        "      frequency[i]+=1\n",
        "\n",
        "# Calculate IDF for each word\n",
        "for i, word in enumerate(all_tokens):\n",
        "    frequency[i] = np.log(len(corpus) / (1 + frequency[i]))\n",
        "    frequency[i]=round(frequency[i],3)\n",
        "for i,word in enumerate(all_tokens):\n",
        "  for j,doc in enumerate(corpus):\n",
        "    TF_IDF_matrix[i][j]=TF_IDF_matrix[i][j]*frequency[i]\n",
        "\n",
        "TF_IDF_matrix\n",
        "df=pd.DataFrame(data=TF_IDF_matrix,columns=[f\"doc{i+1}\"for i in range(len(corpus))])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 2: Using Scikit-Learn to Generate TF-IDF Matrix\n",
        "Objective: Use Scikit-Learn's TfidfVectorizer to create a TF-IDF matrix.\n",
        "Task:\n",
        "Install the necessary libraries: scikit-learn.\n",
        "Load a dataset of text documents, such as a collection of news articles or reviews.\n",
        "Use TfidfVectorizer to convert the collection of text documents to a matrix of TF-IDF features.\n",
        "Print the shape of the resulting matrix and display the first few rows.**"
      ],
      "metadata": {
        "id": "yMzaAu5S0PXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"this is the first document\",\n",
        "    \"this document is the second document\",\n",
        "    \"and this is the third one\",\n",
        "    \"is this the first document\",\n",
        "]\n",
        "Vectorizer=TfidfVectorizer()\n",
        "X=Vectorizer.fit_transform(corpus)\n",
        "\n",
        "df=pd.DataFrame(data=X.toarray(),columns=Vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKm-Aftn_6Kf",
        "outputId": "46c872a5-7a75-432b-deb1-e4d6257112ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        and  document     first        is       one    second       the  \\\n",
            "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
            "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
            "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
            "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
            "\n",
            "      third      this  \n",
            "0  0.000000  0.384085  \n",
            "1  0.000000  0.281089  \n",
            "2  0.511849  0.267104  \n",
            "3  0.000000  0.384085  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment 3: Feature Selection Using TF-IDF\n",
        "Objective: Use TF-IDF for feature selection in a text classification task.\n",
        "Task:\n",
        "Load a labeled text dataset (e.g., spam vs. ham emails).\n",
        "Split the dataset into training and testing sets.\n",
        "Use TfidfVectorizer to transform the text data into TF-IDF features.\n",
        "Train a classification model (e.g., logistic regression, SVM) using the TF-IDF features.\n",
        "Evaluate the model's performance on the testing set.\n",
        "Analyze the importance of features (terms) in the model by examining the coefficients or feature importance scores.**"
      ],
      "metadata": {
        "id": "8IKTkpF4cxOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data=fetch_20newsgroups()\n",
        "X=data.data\n",
        "y=data.target\n",
        "\n",
        "vectorizer=TfidfVectorizer()\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.7,random_state=42)\n",
        "\n",
        "\n",
        "X_train=vectorizer.fit_transform(X_train)\n",
        "X_test=vectorizer.transform(X_test)\n",
        "\n",
        "model=LogisticRegression()\n",
        "model.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "collapsed": true,
        "id": "c91AaQE9FZhf",
        "outputId": "272b0259-980a-4846-b8cf-8611ec60f115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict=model.predict(X_test)\n",
        "print(\"Classification reort\\n:\",classification_report(y_test,y_predict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz5Xi59WVKMf",
        "outputId": "00effa60-68ea-4226-e27c-bfb254fec877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification reort\n",
            ":               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.81      0.86       338\n",
            "           1       0.68      0.79      0.73       401\n",
            "           2       0.78      0.79      0.78       405\n",
            "           3       0.75      0.75      0.75       418\n",
            "           4       0.86      0.76      0.81       410\n",
            "           5       0.78      0.86      0.81       404\n",
            "           6       0.58      0.88      0.70       389\n",
            "           7       0.90      0.83      0.86       431\n",
            "           8       0.90      0.90      0.90       423\n",
            "           9       0.82      0.95      0.88       395\n",
            "          10       0.95      0.94      0.94       403\n",
            "          11       0.97      0.91      0.94       429\n",
            "          12       0.88      0.69      0.77       423\n",
            "          13       0.89      0.89      0.89       413\n",
            "          14       0.93      0.87      0.90       431\n",
            "          15       0.73      0.92      0.81       422\n",
            "          16       0.90      0.87      0.88       398\n",
            "          17       0.93      0.93      0.93       384\n",
            "          18       0.95      0.77      0.85       339\n",
            "          19       0.91      0.39      0.55       264\n",
            "\n",
            "    accuracy                           0.83      7920\n",
            "   macro avg       0.85      0.82      0.83      7920\n",
            "weighted avg       0.85      0.83      0.83      7920\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names=vectorizer.get_feature_names_out()\n",
        "features_coef=list(zip(model.coef_[0],feature_names))"
      ],
      "metadata": {
        "id": "SSEAB14TYt3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_coef_sorted=sorted(features_coef,key=lambda x: abs(x[0]))[-10:]\n",
        "print(\"Top 10 most important features:\")\n",
        "for f_c in features_coef_sorted:\n",
        "  print(f\"{f_c[1]}: {f_c[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W48mbPwFa8p5",
        "outputId": "8740e7e6-70f8-4337-b447-0004c172a956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most important features:\n",
            "that: 1.5803264130112498\n",
            "sgi: 1.680745898223057\n",
            "islam: 1.749634526740061\n",
            "islamic: 1.7581761289974793\n",
            "livesey: 1.768244265495613\n",
            "is: 1.8116080485735389\n",
            "caltech: 1.9970584137394265\n",
            "atheism: 2.1948455907782005\n",
            "god: 2.3629577901778505\n",
            "keith: 3.11031855714001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "# Example documents\n",
        "documents = [\n",
        "    \"Machine learning is fun.\",\n",
        "    \"Machine learning is fun. Machine learning algorithms are important. Machine learning can solve many problems. Machine learning is everywhere.\"\n",
        "]\n",
        "\n",
        "# Step 1: Compute the TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Convert the sparse matrix to a dense matrix (optional, for easier understanding)\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Step 3: Normalize the TF-IDF matrix\n",
        "normalized_tfidf = normalize(tfidf_dense, norm='l2')\n",
        "\n",
        "# Print the results\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_dense)\n",
        "print(\"\\nNormalized TF-IDF Matrix:\")\n",
        "print(normalized_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiimexVvlxjG",
        "outputId": "41d6d4d1-002f-4089-ad06-b2acfec4ec3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.5        0.\n",
            "  0.5        0.5        0.5        0.         0.         0.        ]\n",
            " [0.1934159  0.1934159  0.1934159  0.1934159  0.13761701 0.1934159\n",
            "  0.27523402 0.55046803 0.55046803 0.1934159  0.1934159  0.1934159 ]]\n",
            "\n",
            "Normalized TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.         0.5        0.\n",
            "  0.5        0.5        0.5        0.         0.         0.        ]\n",
            " [0.1934159  0.1934159  0.1934159  0.1934159  0.13761701 0.1934159\n",
            "  0.27523402 0.55046803 0.55046803 0.1934159  0.1934159  0.1934159 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDKmeenfpEao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}