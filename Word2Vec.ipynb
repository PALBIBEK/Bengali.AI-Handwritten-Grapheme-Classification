{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCXdNyxP9jtaOrJwacVpA7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PALBIBEK/Bengali.AI-Handwritten-Grapheme-Classification/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaLHEss_VIIH",
        "outputId": "048d2ce7-0f05-4c3f-cea7-25095f141da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "[['mahantgvcom', 'patrick', 'l', 'mahan', 'subject', 'just', 'newsgroup', 'dead', 'organization', 'internet', 'lines', 'nntppostinghost', 'enterpoopmitedu', 'xpertexpolcsmitedu', 'rlmhelensurfctycom', 'ive', 'gotten', 'posts', 'group', 'couple', 'days', 'recently', 'added', 'feed', 'list', 'just', 'group', 'near', 'death', 'seen', 'mailing', 'list', 'im', 'getting', 'right', 'traffic', 'patrick', 'l', 'mahan', 'tgv', 'window', 'washer', 'mahantgvcom', 'waking', 'person', 'unnecessarily', 'considered', 'lazarus', 'long', 'capital', 'crime', 'offense', 'notebooks', 'lazarus', 'long', 'patrick', 'l', 'mahan', 'tgv', 'window', 'washer', 'mahantgvcom', 'waking', 'person', 'unnecessarily', 'considered', 'lazarus', 'long', 'capital', 'crime', 'offense', 'notebooks', 'lazarus', 'long'], ['maxqueernetorg', 'max', 'j', 'rochlin', 'subject', 'speeding', 'ticket', 'chp', 'organization', 'queernet', 'lines', 'interesting', 'id', 'fight', 'ticket', 'theres', 'chance', 'cop', 'wont', 'secondly', 'does', 'point', 'lied', 'purgered', 'ticket', 'beleive', 'yore', 'charged', 'going', 'mph', 'posted', 'speed', 'severe', 'ticket', 'possibly', 'going', 'right', 'maxqueernetorg', 'max', 'j', 'rochlin', 'uunetsgiunpcmax', 'protect', 'want'], ['mmmcupportalcom', 'mark', 'robert', 'thorson', 'subject', 'centi', 'milli', 'pedes', 'organization', 'portal', 'tm', 'lines', 'remember', 'kid', 'visiting', 'relatives', 'kauai', 'things', 'really', 'frightened', 'centipedes', 'id', 'told', 'poisonous', 'infrequently', 'pop', 'scare', 'heck', 'came', 'vacuum', 'cleaner', 'like', 'foot', 'long', 'moving', 'miles', 'hour'], ['uunetoliveasgigatesgiblabadagiopanasoniccomnntpservercaltechedukeith', 'subject', 'political', 'atheists', 'keithccocaltechedu', 'keith', 'allan', 'schneider', 'organization', 'california', 'institute', 'technology', 'pasadena', 'nntppostinghost', 'punishercaltechedu', 'lines', 'bobbeviceicotekcom', 'robert', 'beauchaine', 'writes', 'personal', 'objection', 'capital', 'punishment', 'cruel', 'unusual', 'punishment', 'circumstances', 'painless', 'isnt', 'cruel', 'occurred', 'frequently', 'dawn', 'time', 'hardly', 'unusual', 'dont', 'issue', 'numbers', 'single', 'innocent', 'life', 'taken', 'innocents', 'die', 'causes', 'singled', 'accidental', 'false', 'execution', 'issue', 'keith'], ['organization', 'university', 'notre', 'dame', 'office', 'univ', 'computing', 'rvestermvmaccndedu', 'subject', 'bob', 'vestermans', 'plan', 'generate', 'fan', 'lines', 'owners', 'whining', 'baseball', 'popular', 'large', 'portion', 'population', 'suggested', 'various', 'remedies', 'shortening', 'game', 'trying', 'convince', 'smokeembakeemdominatebysheerintimidation', 'accurate', 'description', 'essentially', 'laidback', 'game', 'forget', 'lame', 'ideas', 'new', 'exciting', 'twopoint', 'plan', 'generate', 'baseball', 'masses', 'point', 'sex', 'point', 'violence', 'lets', 'face', 'sex', 'violence', 'things', 'sell', 'america', 'heres', 'implement', 'game', 'sex', 'cheerleaders', 'cheerleaders', 'cheerleaders', 'dancing', 'dugouts', 'bringing', 'hot', 'dogs', 'umps', 'seventh', 'inning', 'stretch', 'running', 'stands', 'south', 'bend', 'white', 'sox', 'actually', 'violence', 'baseball', 'players', 'utter', 'wuss', 'boys', 'pitcher', 'beans', 'batter', 'benches', 'called', 'benchclearing', 'brawl', 'everybody', 'just', 'stands', 'looks', 'stand', 'stand', 'stand', 'look', 'look', 'look', 'ho', 'hum', 'bullpens', 'come', 'running', 'reach', 'fight', 'just', 'stand', 'anybody', 'coming', 'bench', 'does', 'throw', 'punch', 'suspended', 'fined', 'bullpens', 'fight', 'outfield', 'waste', 'time', 'energy', 'running', 'infield', 'football', 'sex', 'violence', 'basketball', 'sex', 'violence', 'hockey', 'violence', 'baseball', 'da', 'pastime', 'da', 'nayshun', 'yawn', 'bob', 'vesterman']]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fetch the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all')\n",
        "documents = newsgroups_data.data\n",
        "labels = newsgroups_data.target\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess(text):\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
        "    return text\n",
        "\n",
        "# Preprocess all documents\n",
        "processed_documents = [preprocess(doc) for doc in documents]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_documents, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the training documents\n",
        "tokenized_docs = [word_tokenize(doc) for doc in X_train]\n",
        "\n",
        "print(tokenized_docs[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model on the tokenized documents\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Save the trained Word2Vec model\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "# Load the Word2Vec model (optional, can be used to load a pre-trained model)\n",
        "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Function to get the average word vector for a document\n",
        "def get_document_vector(doc, model):\n",
        "    # Tokenize the document\n",
        "    words = word_tokenize(doc)\n",
        "    # Get vectors for words present in the Word2Vec model\n",
        "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    # Compute the mean vector if there are any word vectors, otherwise return a zero vector\n",
        "    if word_vecs:\n",
        "        return np.mean(word_vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "# Represent each document in the training and testing sets by averaging word vectors\n",
        "X_train_vecs = [get_document_vector(doc, w2v_model) for doc in X_train]\n",
        "X_test_vecs =  ([get_document_vector(doc, w2v_model) for doc in X_test])\n",
        "\n",
        "# Convert document vectors and labels to torch tensors and move them to the GPU\n",
        "X_train_vecs = torch.tensor(X_train_vecs).to(device)\n",
        "X_test_vecs = torch.tensor(X_test_vecs).to(device)\n",
        "y_train = torch.tensor(y_train).to(device)\n",
        "y_test = torch.tensor(y_test).to(device)\n",
        "\n",
        "# Train a logistic regression classifier on the document vectors\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "clf.fit(X_train_vecs.cpu(), y_train.cpu())  # scikit-learn does not support GPU, so move tensors to CPU for training\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = clf.predict(X_test_vecs.cpu())  # Move tensors to CPU for prediction\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(y_test.cpu(), y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(y_test.cpu(), y_pred, target_names=newsgroups_data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBleDgDHv7J7",
        "outputId": "a14fecf0-0a99-49d4-c054-545d8cbfda06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5a055a1d13be>:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  X_train_vecs = torch.tensor(X_train_vecs).to(device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6594\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.54      0.56      0.55       151\n",
            "           comp.graphics       0.54      0.57      0.55       202\n",
            " comp.os.ms-windows.misc       0.65      0.65      0.65       195\n",
            "comp.sys.ibm.pc.hardware       0.48      0.52      0.50       183\n",
            "   comp.sys.mac.hardware       0.59      0.39      0.47       205\n",
            "          comp.windows.x       0.73      0.73      0.73       215\n",
            "            misc.forsale       0.63      0.69      0.66       193\n",
            "               rec.autos       0.63      0.64      0.64       196\n",
            "         rec.motorcycles       0.58      0.78      0.67       168\n",
            "      rec.sport.baseball       0.73      0.65      0.69       211\n",
            "        rec.sport.hockey       0.72      0.81      0.76       198\n",
            "               sci.crypt       0.86      0.88      0.87       201\n",
            "         sci.electronics       0.61      0.48      0.54       202\n",
            "                 sci.med       0.70      0.76      0.73       194\n",
            "               sci.space       0.74      0.86      0.80       189\n",
            "  soc.religion.christian       0.68      0.88      0.77       202\n",
            "      talk.politics.guns       0.66      0.75      0.70       188\n",
            "   talk.politics.mideast       0.78      0.78      0.78       182\n",
            "      talk.politics.misc       0.59      0.48      0.53       159\n",
            "      talk.religion.misc       0.60      0.15      0.25       136\n",
            "\n",
            "                accuracy                           0.66      3770\n",
            "               macro avg       0.65      0.65      0.64      3770\n",
            "            weighted avg       0.66      0.66      0.65      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eALXisxY2Nus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_vecs.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m6PxMDI0Ubi",
        "outputId": "085a129c-0292-4304-8785-878ff7a585a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15076, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fetch the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all')\n",
        "documents = newsgroups_data.data\n",
        "labels = newsgroups_data.target\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])  # Remove stop words\n",
        "    return text\n",
        "\n",
        "# Preprocess all documents with tqdm progress bar\n",
        "processed_documents = [preprocess(doc) for doc in tqdm(documents, desc=\"Preprocessing documents\")]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_documents, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the training documents with tqdm progress bar\n",
        "tokenized_docs = [word_tokenize(doc) for doc in tqdm(X_train, desc=\"Tokenizing documents\")]\n",
        "\n",
        "# Train the Word2Vec model on the tokenized documents with tqdm progress bar\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
        "w2v_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=10)\n",
        "\n",
        "# Save the trained Word2Vec model\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "# Load the Word2Vec model (optional, can be used to load a pre-trained model)\n",
        "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNDocumentEmbedder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNNDocumentEmbedder, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, word_vecs):\n",
        "        word_vecs = torch.tensor(word_vecs).float().unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "        _, (hn, _) = self.rnn(word_vecs)\n",
        "        return hn.squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "# Initialize the RNN embedder\n",
        "embedder = RNNDocumentEmbedder(input_size=w2v_model.vector_size, hidden_size=100).to(device)\n",
        "\n",
        "# Function to get the document vector using the RNN\n",
        "def get_document_vector_rnn(doc, model, embedder):\n",
        "    words = word_tokenize(doc)\n",
        "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if word_vecs:\n",
        "        return embedder(word_vecs)\n",
        "    else:\n",
        "        return np.zeros(embedder.hidden_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqpK8Sza2Pwu",
        "outputId": "ca70e62d-e5b4-402f-acaf-a6e83072c0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing documents: 100%|██████████| 18846/18846 [00:09<00:00, 1960.85it/s]\n",
            "Tokenizing documents: 100%|██████████| 15076/15076 [00:11<00:00, 1260.44it/s]\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent each document in the training and testing sets by processing through the RNN\n",
        "X_train_vecs = np.array([get_document_vector_rnn(doc, w2v_model, embedder) for doc in tqdm(X_train, desc=\"Vectorizing train documents\")])\n",
        "X_test_vecs = np.array([get_document_vector_rnn(doc, w2v_model, embedder) for doc in tqdm(X_test, desc=\"Vectorizing test documents\")])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyzOY2y1Cxd3",
        "outputId": "fb823ba5-442c-4bff-b51d-f22d927d4810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Vectorizing train documents: 100%|██████████| 15076/15076 [01:41<00:00, 148.30it/s]\n",
            "Vectorizing test documents: 100%|██████████| 3770/3770 [00:25<00:00, 149.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shapes of the document vectors\n",
        "print(f\"Shape of X_train_vecs: {X_train_vecs.shape}\")\n",
        "print(f\"Shape of X_test_vecs: {X_test_vecs.shape}\")\n",
        "X_train_vecs=X_train_vecs.squeeze(1)\n",
        "X_test_vecs=X_test_vecs.squeeze(1)\n",
        "# Train a logistic regression classifier on the document vectors\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "clf.fit(X_train_vecs, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = clf.predict(X_test_vecs)\n",
        "\n",
        "# Evaluate the classifier's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups_data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP7I4u5tE-ma",
        "outputId": "7a37a279-3ff8-44ff-d3aa-db6f519ca568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_vecs: (15076, 1, 100)\n",
            "Shape of X_test_vecs: (3770, 1, 100)\n",
            "Accuracy: 0.3045\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.42      0.45      0.43       151\n",
            "           comp.graphics       0.18      0.15      0.16       202\n",
            " comp.os.ms-windows.misc       0.20      0.10      0.13       195\n",
            "comp.sys.ibm.pc.hardware       0.18      0.19      0.18       183\n",
            "   comp.sys.mac.hardware       0.19      0.10      0.13       205\n",
            "          comp.windows.x       0.23      0.21      0.22       215\n",
            "            misc.forsale       0.28      0.35      0.31       193\n",
            "               rec.autos       0.27      0.31      0.29       196\n",
            "         rec.motorcycles       0.33      0.41      0.36       168\n",
            "      rec.sport.baseball       0.31      0.32      0.31       211\n",
            "        rec.sport.hockey       0.41      0.55      0.47       198\n",
            "               sci.crypt       0.32      0.42      0.36       201\n",
            "         sci.electronics       0.16      0.08      0.11       202\n",
            "                 sci.med       0.24      0.26      0.25       194\n",
            "               sci.space       0.28      0.41      0.34       189\n",
            "  soc.religion.christian       0.36      0.54      0.43       202\n",
            "      talk.politics.guns       0.39      0.39      0.39       188\n",
            "   talk.politics.mideast       0.51      0.58      0.54       182\n",
            "      talk.politics.misc       0.30      0.17      0.22       159\n",
            "      talk.religion.misc       0.29      0.10      0.14       136\n",
            "\n",
            "                accuracy                           0.30      3770\n",
            "               macro avg       0.29      0.30      0.29      3770\n",
            "            weighted avg       0.29      0.30      0.29      3770\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fetch the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all')\n",
        "documents = newsgroups_data.data\n",
        "labels = newsgroups_data.target\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])  # Remove stop words\n",
        "    return text\n",
        "\n",
        "# Preprocess all documents with tqdm progress bar\n",
        "processed_documents = [preprocess(doc) for doc in tqdm(documents, desc=\"Preprocessing documents\")]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_documents, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the training documents with tqdm progress bar\n",
        "tokenized_docs = [word_tokenize(doc) for doc in tqdm(X_train, desc=\"Tokenizing documents\")]\n",
        "\n",
        "# Train the Word2Vec model on the tokenized documents with tqdm progress bar\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
        "w2v_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=10)\n",
        "\n",
        "# Save the trained Word2Vec model\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "# Load the Word2Vec model (optional, can be used to load a pre-trained model)\n",
        "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNDocumentEmbedder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(RNNDocumentEmbedder, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, word_vecs):\n",
        "        word_vecs = torch.tensor(word_vecs).float().unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "        _, (hn, _) = self.rnn(word_vecs)\n",
        "        return hn.squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "# Initialize the RNN embedder\n",
        "embedder = RNNDocumentEmbedder(input_size=w2v_model.vector_size, hidden_size=100).to(device)\n",
        "\n",
        "# Function to get the document vector using the RNN\n",
        "def get_document_vector_rnn(doc, model, embedder):\n",
        "    words = word_tokenize(doc)\n",
        "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
        "    if word_vecs:\n",
        "        return embedder(word_vecs)\n",
        "    else:\n",
        "        return np.zeros(embedder.hidden_size)\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, documents, labels, model, embedder):\n",
        "        self.documents = documents\n",
        "        self.labels = labels\n",
        "        self.model = model\n",
        "        self.embedder = embedder\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        doc = self.documents[idx]\n",
        "        label = self.labels[idx]\n",
        "        doc_vector = get_document_vector_rnn(doc, self.model, self.embedder)\n",
        "        return torch.tensor(doc_vector, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = TextDataset(X_train, y_train, w2v_model, embedder)\n",
        "test_dataset = TextDataset(X_test, y_test, w2v_model, embedder)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, word_vecs):\n",
        "        _, (hn, _) = self.rnn(word_vecs)\n",
        "        out = self.fc(hn.squeeze(0))\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "num_classes = len(newsgroups_data.target_names)\n",
        "model = RNNClassifier(input_size=w2v_model.vector_size, hidden_size=100, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(all_labels, all_preds, target_names=newsgroups_data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfg3ogcN4pNn",
        "outputId": "74f249cd-10e6-4eb8-8baa-235328e8bc24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing documents: 100%|██████████| 18846/18846 [00:02<00:00, 8613.51it/s]\n",
            "Tokenizing documents: 100%|██████████| 15076/15076 [00:11<00:00, 1365.91it/s]\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "Training Epoch 1/10:   0%|          | 0/472 [00:00<?, ?it/s]<ipython-input-1-b21d6048d357>:63: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  word_vecs = torch.tensor(word_vecs).float().unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
            "Training Epoch 1/10: 100%|██████████| 472/472 [01:33<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 2.7790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 472/472 [01:30<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 2.4865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 472/472 [01:40<00:00,  4.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 2.4002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 472/472 [01:33<00:00,  5.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 2.3473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 472/472 [01:31<00:00,  5.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 2.3092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/10: 100%|██████████| 472/472 [01:30<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 2.2731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/10: 100%|██████████| 472/472 [01:31<00:00,  5.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 2.2405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/10: 100%|██████████| 472/472 [01:31<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 2.2114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/10: 100%|██████████| 472/472 [01:30<00:00,  5.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 2.1817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10/10: 100%|██████████| 472/472 [01:30<00:00,  5.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 2.1528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 118/118 [00:24<00:00,  4.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.3427\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.45      0.46      0.45       151\n",
            "           comp.graphics       0.19      0.16      0.17       202\n",
            " comp.os.ms-windows.misc       0.17      0.11      0.13       195\n",
            "comp.sys.ibm.pc.hardware       0.16      0.13      0.14       183\n",
            "   comp.sys.mac.hardware       0.22      0.12      0.16       205\n",
            "          comp.windows.x       0.19      0.27      0.22       215\n",
            "            misc.forsale       0.34      0.34      0.34       193\n",
            "               rec.autos       0.31      0.39      0.34       196\n",
            "         rec.motorcycles       0.36      0.45      0.40       168\n",
            "      rec.sport.baseball       0.40      0.31      0.35       211\n",
            "        rec.sport.hockey       0.44      0.58      0.50       198\n",
            "               sci.crypt       0.38      0.46      0.42       201\n",
            "         sci.electronics       0.18      0.13      0.16       202\n",
            "                 sci.med       0.28      0.30      0.29       194\n",
            "               sci.space       0.38      0.46      0.42       189\n",
            "  soc.religion.christian       0.42      0.59      0.49       202\n",
            "      talk.politics.guns       0.45      0.57      0.50       188\n",
            "   talk.politics.mideast       0.62      0.65      0.64       182\n",
            "      talk.politics.misc       0.39      0.27      0.32       159\n",
            "      talk.religion.misc       0.40      0.12      0.18       136\n",
            "\n",
            "                accuracy                           0.34      3770\n",
            "               macro avg       0.34      0.34      0.33      3770\n",
            "            weighted avg       0.33      0.34      0.33      3770\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "\n",
        "data=fetch_20newsgroups()\n",
        "X=data.data\n",
        "y=data.target\n",
        "\n",
        "def preprocess(text):\n",
        "  text=re.sub(r'\\d+',' ',text)\n",
        "  text = ''.join([char for char in text if char not in string.punctuation])\n",
        "  text=text.lower()\n",
        "  text=' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
        "  return text\n",
        "\n",
        "preprocess_texts=[preprocess(text) for text in X]\n",
        "preprocess_texts[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMcw4Rq5IAlu",
        "outputId": "d51d0f7b-460a-4dc3-e921-7be48f2d0c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lerxstwamumdedu wheres thing subject car nntppostinghost rac wamumdedu organization university maryland college park lines wondering enlighten car saw day door sports car looked late s early s called bricklin doors really small addition bumper separate rest body know tellme model engine specs years production car history info funky looking car email thanks il brought neighborhood lerxst',\n",
              " 'guykuocarsonuwashingtonedu guy kuo subject si clock poll final summary final si clock reports keywords siaccelerationclockupgrade articleid shelley qvfo innc s organization university washington lines nntppostinghost carsonuwashingtonedu fair number brave souls upgraded si clock oscillator shared experiences poll send brief message detailing experiences procedure speed attained cpu rated speed add cards adapters heat sinks hour usage day floppy disk functionality m floppies especially requested summarizing days add network knowledge base clock upgrade havent answered poll thanks guy kuo guykuouwashingtonedu',\n",
              " 'twillisececnpurdueedu thomas e willis subject pb questions organization purdue university engineering computer network distribution usa lines folks mac plus finally gave ghost weekend starting life k way sooo im market new machine bit sooner intended im looking picking powerbook maybe bunch questions hopefully somebody answer does anybody know dirt round powerbook introductions expected id heard c supposed make appearence summer havent heard anymore dont access macleak wondering anybody info anybody heard rumors price drops powerbook line like ones duos just went recently whats impression display probably swing got mb disk dont really feel better display yea looks great store wow really good solicit opinions people use daytoday worth taking disk size money hit active display realize real subjective question ive played machines computer store breifly figured opinions somebody actually uses machine daily prove helpful does hellcats perform thanks bunch advance info email ill post summary news reading time premium finals just corner tom willis twillisecnpurdueedu purdue electrical engineering convictions dangerous enemies truth lies f w nietzsche',\n",
              " 'jgreenamber joe green subject weitek p organization harris computer systems division lines distribution world nntppostinghost amberssdcsdharriscom xnewsreader tin version pl robert jc kyanko robrjckuucp wrote abraxisiastateedu writes article abraxis class iastateedu know weitek p graphics chip far lowlevel stuff goes looks pretty nice got quadrilateral command requires just points weiteks addressphone number id like information chip joe green harris corporation jgreencsdharriscom computer systems division thing really scares person sense humor jonathan winters',\n",
              " 'jcmheadcfaharvardedu jonathan mcdowell subject shuttle launch question organization smithsonian astrophysical observatory cambridge ma usa distribution sci lines article c owcbn pworldstdcom tombakerworldstdcom tom baker article c jlwx h cscmuedu etratttacs ttuedu pack rat writes clear caution warning memory verify unexpected errors wondering expected error sorry really dumb question parity errors memory previously known conditions waivered yes error knew id curious real meaning quote tom understanding expected errors basically known bugs warning software things checked dont right values arent set till launch suchlike fix code possibly introduce new bugs just tell crew ok warning liftoff ignore jonathan']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Fetch the dataset\n",
        "data = fetch_20newsgroups()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "def preprocess(text):\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    # Remove punctuation\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stop words\n",
        "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to all texts\n",
        "preprocessed_texts = [preprocess(text) for text in X]\n",
        "print(preprocessed_texts[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hknLO7oGMWyR",
        "outputId": "1ef91d0b-5c72-44ce-b762-dbfbbee874a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lerxstwamumdedu wheres thing subject car nntppostinghost rac wamumdedu organization university maryland college park lines wondering enlighten car saw day door sports car looked late s early s called bricklin doors really small addition bumper separate rest body know tellme model engine specs years production car history info funky looking car email thanks il brought neighborhood lerxst', 'guykuocarsonuwashingtonedu guy kuo subject si clock poll final summary final si clock reports keywords siaccelerationclockupgrade articleid shelley qvfo innc s organization university washington lines nntppostinghost carsonuwashingtonedu fair number brave souls upgraded si clock oscillator shared experiences poll send brief message detailing experiences procedure speed attained cpu rated speed add cards adapters heat sinks hour usage day floppy disk functionality m floppies especially requested summarizing days add network knowledge base clock upgrade havent answered poll thanks guy kuo guykuouwashingtonedu', 'twillisececnpurdueedu thomas e willis subject pb questions organization purdue university engineering computer network distribution usa lines folks mac plus finally gave ghost weekend starting life k way sooo im market new machine bit sooner intended im looking picking powerbook maybe bunch questions hopefully somebody answer does anybody know dirt round powerbook introductions expected id heard c supposed make appearence summer havent heard anymore dont access macleak wondering anybody info anybody heard rumors price drops powerbook line like ones duos just went recently whats impression display probably swing got mb disk dont really feel better display yea looks great store wow really good solicit opinions people use daytoday worth taking disk size money hit active display realize real subjective question ive played machines computer store breifly figured opinions somebody actually uses machine daily prove helpful does hellcats perform thanks bunch advance info email ill post summary news reading time premium finals just corner tom willis twillisecnpurdueedu purdue electrical engineering convictions dangerous enemies truth lies f w nietzsche', 'jgreenamber joe green subject weitek p organization harris computer systems division lines distribution world nntppostinghost amberssdcsdharriscom xnewsreader tin version pl robert jc kyanko robrjckuucp wrote abraxisiastateedu writes article abraxis class iastateedu know weitek p graphics chip far lowlevel stuff goes looks pretty nice got quadrilateral command requires just points weiteks addressphone number id like information chip joe green harris corporation jgreencsdharriscom computer systems division thing really scares person sense humor jonathan winters', 'jcmheadcfaharvardedu jonathan mcdowell subject shuttle launch question organization smithsonian astrophysical observatory cambridge ma usa distribution sci lines article c owcbn pworldstdcom tombakerworldstdcom tom baker article c jlwx h cscmuedu etratttacs ttuedu pack rat writes clear caution warning memory verify unexpected errors wondering expected error sorry really dumb question parity errors memory previously known conditions waivered yes error knew id curious real meaning quote tom understanding expected errors basically known bugs warning software things checked dont right values arent set till launch suchlike fix code possibly introduce new bugs just tell crew ok warning liftoff ignore jonathan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "data=fetch_20newsgroups()\n",
        "X=data.data\n",
        "y=data.target\n",
        "\n",
        "def process(text):\n",
        "\n",
        "  text=re.sub(r'\\d+', ' ',text)\n",
        "  text=''.join([chars for chars in text if chars not in string.punctuation])\n",
        "  text=text.lower()\n",
        "  text=' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
        "  return text\n",
        "\n",
        "\n",
        "\n",
        "process_text=[process(text) for text in X]\n",
        "process_text[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQC4bQVwPLA9",
        "outputId": "9b2b5c62-1f19-4833-ba43-2bdb0366060a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lerxstwamumdedu wheres thing subject car nntppostinghost rac wamumdedu organization university maryland college park lines wondering enlighten car saw day door sports car looked late s early s called bricklin doors really small addition bumper separate rest body know tellme model engine specs years production car history info funky looking car email thanks il brought neighborhood lerxst',\n",
              " 'guykuocarsonuwashingtonedu guy kuo subject si clock poll final summary final si clock reports keywords siaccelerationclockupgrade articleid shelley qvfo innc s organization university washington lines nntppostinghost carsonuwashingtonedu fair number brave souls upgraded si clock oscillator shared experiences poll send brief message detailing experiences procedure speed attained cpu rated speed add cards adapters heat sinks hour usage day floppy disk functionality m floppies especially requested summarizing days add network knowledge base clock upgrade havent answered poll thanks guy kuo guykuouwashingtonedu',\n",
              " 'twillisececnpurdueedu thomas e willis subject pb questions organization purdue university engineering computer network distribution usa lines folks mac plus finally gave ghost weekend starting life k way sooo im market new machine bit sooner intended im looking picking powerbook maybe bunch questions hopefully somebody answer does anybody know dirt round powerbook introductions expected id heard c supposed make appearence summer havent heard anymore dont access macleak wondering anybody info anybody heard rumors price drops powerbook line like ones duos just went recently whats impression display probably swing got mb disk dont really feel better display yea looks great store wow really good solicit opinions people use daytoday worth taking disk size money hit active display realize real subjective question ive played machines computer store breifly figured opinions somebody actually uses machine daily prove helpful does hellcats perform thanks bunch advance info email ill post summary news reading time premium finals just corner tom willis twillisecnpurdueedu purdue electrical engineering convictions dangerous enemies truth lies f w nietzsche',\n",
              " 'jgreenamber joe green subject weitek p organization harris computer systems division lines distribution world nntppostinghost amberssdcsdharriscom xnewsreader tin version pl robert jc kyanko robrjckuucp wrote abraxisiastateedu writes article abraxis class iastateedu know weitek p graphics chip far lowlevel stuff goes looks pretty nice got quadrilateral command requires just points weiteks addressphone number id like information chip joe green harris corporation jgreencsdharriscom computer systems division thing really scares person sense humor jonathan winters',\n",
              " 'jcmheadcfaharvardedu jonathan mcdowell subject shuttle launch question organization smithsonian astrophysical observatory cambridge ma usa distribution sci lines article c owcbn pworldstdcom tombakerworldstdcom tom baker article c jlwx h cscmuedu etratttacs ttuedu pack rat writes clear caution warning memory verify unexpected errors wondering expected error sorry really dumb question parity errors memory previously known conditions waivered yes error knew id curious real meaning quote tom understanding expected errors basically known bugs warning software things checked dont right values arent set till launch suchlike fix code possibly introduce new bugs just tell crew ok warning liftoff ignore jonathan']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(process_text,y,test_size=0.2,random_state=True)\n",
        "\n",
        "all_sentence_all_tokens=[]\n",
        "for text in X_train:\n",
        "  all_sentence_all_tokens.append(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "7V16e7Fweg7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec=Word2Vec(all_sentence_all_tokens,vector_size=100)\n",
        "word2vec.train(all_sentence_all_tokens,total_examples=len(all_sentence_all_tokens),epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPvxKNY6l8vh",
        "outputId": "54d92abe-82d9-42f2-d49a-09e51bf7d5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12056576, 13735890)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Fetch the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all')\n",
        "documents = newsgroups_data.data\n",
        "labels = newsgroups_data.target\n",
        "\n",
        "# Function to preprocess text data\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = ' '.join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])  # Remove stop words\n",
        "    return text\n",
        "\n",
        "# Preprocess all documents with tqdm progress bar\n",
        "processed_documents = [preprocess(doc) for doc in tqdm(documents, desc=\"Preprocessing documents\")]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_documents, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the training documents with tqdm progress bar\n",
        "tokenized_docs = [word_tokenize(doc) for doc in tqdm(X_train, desc=\"Tokenizing documents\")]\n",
        "\n",
        "# Train the Word2Vec model on the tokenized documents with tqdm progress bar\n",
        "w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=2, workers=4, epochs=10)\n",
        "w2v_model.train(tokenized_docs, total_examples=len(tokenized_docs), epochs=10)\n",
        "\n",
        "# Save the trained Word2Vec model\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "# Load the Word2Vec model (optional, can be used to load a pre-trained model)\n",
        "w2v_model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Define the improved combined model\n",
        "class ImprovedRNNClassifier(nn.Module):\n",
        "    def __init__(self, embedding_size, hidden_size, num_classes, dropout_prob=0.5):\n",
        "        super(ImprovedRNNClassifier, self).__init__()\n",
        "        self.embedder = nn.LSTM(embedding_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, word_vecs):\n",
        "        _, (hn, _) = self.embedder(word_vecs)\n",
        "        hn = torch.cat((hn[0], hn[1]), dim=1)  # Concatenate the outputs of the forward and backward LSTMs\n",
        "        out = self.fc1(hn)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Define a custom dataset class\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, documents, labels, model):\n",
        "        self.documents = documents\n",
        "        self.labels = labels\n",
        "        self.model = model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        doc = self.documents[idx]\n",
        "        label = self.labels[idx]\n",
        "        words = word_tokenize(doc)\n",
        "        word_vecs = [self.model.wv[word] for word in words if word in self.model.wv]\n",
        "        if not word_vecs:\n",
        "            word_vecs = [np.zeros(self.model.vector_size)]\n",
        "        word_vecs = torch.tensor(word_vecs, dtype=torch.float32)\n",
        "        return word_vecs, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = TextDataset(X_train, y_train, w2v_model)\n",
        "test_dataset = TextDataset(X_test, y_test, w2v_model)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize the improved model, loss function, and optimizer\n",
        "num_classes = len(newsgroups_data.target_names)\n",
        "improved_model = ImprovedRNNClassifier(embedding_size=w2v_model.vector_size, hidden_size=100, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(improved_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    improved_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = improved_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "improved_model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = improved_model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(all_labels, all_preds, target_names=newsgroups_data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "hVSTYzJrSdEw",
        "outputId": "bc10bddd-9750-4180-c584-0cb7adeb5f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocessing documents: 100%|██████████| 18846/18846 [00:10<00:00, 1835.38it/s]\n",
            "Tokenizing documents: 100%|██████████| 15076/15076 [00:18<00:00, 834.96it/s]\n",
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
            "Training Epoch 1/10:   0%|          | 0/472 [00:00<?, ?it/s]<ipython-input-1-7bb3d4c28644>:91: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
            "  word_vecs = torch.tensor(word_vecs, dtype=torch.float32)\n",
            "Training Epoch 1/10:   0%|          | 0/472 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [314, 100] at entry 0 and [45, 100] at entry 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7bb3d4c28644>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mimproved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [314, 100] at entry 0 and [45, 100] at entry 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYa9eIs0gfep",
        "outputId": "206af373-a3f8-4265-9042-69cb8eaf6040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkfA3Pcbf8h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}